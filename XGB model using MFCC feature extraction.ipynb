{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4213460,"sourceType":"datasetVersion","datasetId":2483929}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndata_csv = pd.read_csv('/kaggle/input/fsc22-dataset/Metadata-20220916T202011Z-001/Metadata/Metadata V1.0 FSC22.csv')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-26T11:06:29.821127Z","iopub.execute_input":"2026-01-26T11:06:29.821399Z","iopub.status.idle":"2026-01-26T11:06:31.674644Z","shell.execute_reply.started":"2026-01-26T11:06:29.821372Z","shell.execute_reply":"2026-01-26T11:06:31.673560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_csv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-26T11:06:31.676804Z","iopub.execute_input":"2026-01-26T11:06:31.677119Z","iopub.status.idle":"2026-01-26T11:06:31.718596Z","shell.execute_reply.started":"2026-01-26T11:06:31.677089Z","shell.execute_reply":"2026-01-26T11:06:31.717607Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have 2025 data sets as our audio data sets.\n\nto Augment the data set and widen it we pitch 2 steps up and down in each audio file","metadata":{}},{"cell_type":"code","source":"import librosa\nimport soundfile as sf\nfrom tqdm import tqdm\nimport os\n\ninput_dir = \"/kaggle/input/fsc22-dataset/Audio Wise V1.0-20220916T202003Z-001/Audio Wise V1.0\"\n\noutput_dir = \"/kaggle/working/FSC22_augmented\"\nos.makedirs(output_dir, exist_ok = True)\n\n\nPITCH_STEPS = [2,-2]\n\nfor file in tqdm(os.listdir(input_dir)):\n    if file.endswith(\".wav\"):\n        file_path = os.path.join(input_dir, file)\n\n        y, sr = librosa.load(file_path, sr=None) #loading the audio\n\n        sf.write(os.path.join(output_dir, file),y,sr)\n\n        for step in PITCH_STEPS:\n            y_shifted = librosa.effects.pitch_shift(y,sr= sr, n_steps=step)\n            base, ext = os.path.splitext(file)\n            new_filename=f\"{base}_pitch{step}{ext}\"\n            sf.write(os.path.join(output_dir, new_filename), y_shifted, sr)\n\nprint(\"Augmentation Completed!\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T19:31:40.584145Z","iopub.execute_input":"2026-01-27T19:31:40.584461Z","iopub.status.idle":"2026-01-27T19:38:46.166159Z","shell.execute_reply.started":"2026-01-27T19:31:40.584407Z","shell.execute_reply":"2026-01-27T19:38:46.165444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filename_to_label= {}\n\nfor i, row in data_csv.iterrows():\n    filename = row[\"Daraset File name\"]\n    class_id = row[\"Class ID\"]\n    filename_to_label[filename] = class_id\n\ndef get_orginal_filename(filename):\n    base, ext = os.path.splitext(filename)\n    if \"_pitch\" in base:\n        base = base.split(\"_pitch\")[0]\n    return base + ext\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since we have augmented the data set now we are trying to do feature extraction.\n\nfor ausio signal processing, we use MFCC and melspectrogram methods using librosa library","metadata":{}},{"cell_type":"code","source":"\nauto_dir = \"/kaggle/working/FSC22_augmented\"\n\n#Feature Extraction Parameters\n\nSR = 22050 #sampling rate\nN_FFT = 2048  #number of samples per FFT window\nHOP_LENGTH = 512 #\nN_MELS = 128 #number of mel frequency bands. 128 is ide\nN_MFCC = 13 #number of MFCC coefficients\n\ndef extract_mel_spectrogram(file_path):\n    y, sr = librosa.load(file_path, sr=SR) # Y is the 1D waveform array\n    mel_spec = librosa.feature.melspectrogram( y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mel=N_MELS)\n    mel_specs_db = librosa.power_to_db(mel_spec. ref=np.max)\n\n    return mel_spec_db\n\ndef extract_mfcc(file_path):\n    y, sr = librosa.load(file_path, sr=SR)\n    mfcc = librosa.feature.mfcc(y=y, sr=sr,n_mfcc=N_MFCC,n_fft=N_FFT,hop_length=HOP_LENGTH)\n\n    #MFCC algorthm works as.. STFT ->MEL filterbank(triangular) -> log -> DCT\n    return mfcc\n\n\n\n#Looping \n\n\nmel_features = []\nmfcc_features = []\nfile_names = []\n\nfor file in tqdm(os.listdir(audio_dir)):\n    if file.endswith(\".wav\"):\n\n        path = os.path.join(auto_dir, file):\n\n        mel =extract_mel_spectrogram(path)\n        mfcc = extract_mfcc(path)\n\n        mel_features.append(mel)\n        mfcc_features.append(mfcc)\n        file_names.append(file)\n\n        mel_features = np.array(mel_fetures, dtype = object)\n        mfcc_features = np.array(mfcc_features, dtype = object)\n\n\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfromtqdm import tqdm\nimport librosa\n\nx = []\ny = []\n\n#preparing 1D feature \n\nfor mfcc, fname in zip(mfcc_features, file_names):\n    orginal_fname = get_orginal_filename(fname)\n    label = filename_to_label[orginal_fname]\n\n    mfcc_mean = np.mean(mfcc,axis =1)\n    x.append(mfcc_mean)\n    y.append(label)\n    \n\nfrom sklearn.preprocessing import LabelEncoder\n\nle  = LabelEncoder\ny_encoded = le.fit_transform(y)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split( X, y_encoded, test_size=0.2, random_state=42, stratify = y)\n\nimport xgboost as xgb\n\ndtrain =xgb.DMatrix( X_train, label = y_train)\ndval = xgb.Dmatrix(X_val, label=y_val)\n\nparams = {\n    \"objective\": \"multi:softmax\",\n    \"num_class\" : 27,\n    \"eval_metric\":\"merror\"\n    \"subsample\":1,\n    \"min_child_weight\":1,\n    \"max_depth\":6,\n    \"learning_rate\":0.3\n}\n\nnum_rounds = 100\n\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_roubd = num_rounds,\n    evals=[(dval, \"validation\")])\n)\n\npreds = model.predict(dval)\naccuracy = (preds==y_val).mean()\nprint(\"validation _accuracy:\", accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}